{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom1 = []\n",
    "dom2 = []\n",
    "Test_set = []\n",
    "with open('../data/domain1_train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dom1.append(json.loads(line))\n",
    "        \n",
    "with open('../data/domain2_train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dom2.append(json.loads(line))\n",
    "        \n",
    "sam = pd.read_csv(\"../data/sample.csv\")\n",
    "\n",
    "with open('../data/test_set.json', 'r') as file:\n",
    "    for line in file:\n",
    "        Test_set.append(json.loads(line))\n",
    "        \n",
    "dom1 = pd.DataFrame.from_dict(dom1)\n",
    "dom2 = pd.DataFrame.from_dict(dom2)\n",
    "dom2 = dom2[dom2['text'].apply(len) > 0]\n",
    "Test_set = pd.DataFrame.from_dict(Test_set)\n",
    "\n",
    "# human as model 8, dom1 AI as model 7\n",
    "dom1['model'] = np.where(dom1['label'] == 1, 8, 7)\n",
    "dom2[\"model\"] = dom2[\"model\"].replace(np.nan, 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resample "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. full dom1 + even dom2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom2_0 = dom2.loc[dom2['label']==0]\n",
    "dom2_1 = dom2.loc[dom2['label']==1]\n",
    "\n",
    "length_dom2_1 = len(dom2_1)\n",
    "mol_num = len(dom2['model'].value_counts())\n",
    "least_mol_num = length_dom2_1/(mol_num-1)\n",
    "sam_dom2_0 = dom2_0.groupby('model').apply(lambda x: x.sample(n=int(least_mol_num)))\n",
    "sam_dom2_0.reset_index(drop=True, inplace=True)\n",
    "dom2_even = result = pd.concat([dom2_1, sam_dom2_0])\n",
    "\n",
    "df = pd.concat([dom1, dom2_even])\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "8.0    11899\n",
       "7.0     9750\n",
       "1.0      307\n",
       "2.0      307\n",
       "3.0      307\n",
       "4.0      307\n",
       "6.0      307\n",
       "5.0      307\n",
       "0.0      307\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['model'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. model1-7 same langth, even label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_all = pd.concat([dom1, dom2])\n",
    "# df_all0 = df_all.loc[df_all['label']==0]\n",
    "# df_all1 = df_all.loc[df_all['label']==1]\n",
    "# mol_num = len(df_all0['model'].value_counts())\n",
    "# least_mol_num = df_all0['model'].value_counts().tolist()[-1]\n",
    "# sam_df_all0 = df_all0.groupby('model').apply(lambda x: x.sample(n=int(least_mol_num)))\n",
    "# sam_df_all1 = df_all1.sample(n=least_mol_num*mol_num, random_state=42)\n",
    "# df = pd.concat([sam_df_all0, sam_df_all1])\n",
    "# df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "8.0    6240\n",
       "4.0     780\n",
       "7.0     780\n",
       "5.0     780\n",
       "0.0     780\n",
       "2.0     780\n",
       "6.0     780\n",
       "3.0     780\n",
       "1.0     780\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['model'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new feature first \n",
    "dom1['text_length'] = dom1['text'].apply(lambda x: len(x))\n",
    "dom2['text_length'] = dom2['text'].apply(lambda x: len(x))\n",
    "\n",
    "minimax_scale = MinMaxScaler()\n",
    "dom1['text_length'] = minimax_scale.fit_transform(dom1[['text_length']])\n",
    "dom2['text_length'] = minimax_scale.fit_transform(dom2[['text_length']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = []\n",
    "df_merge = pd.concat([dom1,dom2])\n",
    "AI_num = 8\n",
    "df_all0 = df_merge.loc[df_merge['label']==0]\n",
    "df_all1= df_merge.loc[df_merge['label']==1]\n",
    "least_mod_num = df_all0['model'].value_counts().tolist()[-1]\n",
    "for _ in range(5):\n",
    "    df_all0_even = df_all0.groupby('model').apply(lambda x: x.sample(n=int(least_mod_num)))\n",
    "    df_all0_even.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_all1_equal = df_all1.groupby('model').apply(lambda x: x.sample(n=int(least_mod_num*AI_num)))\n",
    "    df_merge = pd.concat([df_all0_even,df_all1_equal])\n",
    "    df_merge = df_merge.sample(frac=1).reset_index(drop=True)\n",
    "    df_all.append(df_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_dfs1 = []\n",
    "# num_iterations = 5\n",
    "# AI_num = 8\n",
    "# rows_per_iteration = int(least_mod_num*AI_num)\n",
    "# total_rows = num_iterations * rows_per_iteration\n",
    "# # Create a list to keep track of which rows have been selected\n",
    "# selected_indices = []\n",
    "# ind = []\n",
    "# for _ in range(num_iterations):\n",
    "#     # If all rows have been selected, randomly choose to keep rows\n",
    "#     if len(selected_indices) >= len(dom1):\n",
    "#         random_indices = np.random.choice(len(df_all1), rows_per_iteration, replace=False)\n",
    "#     else:\n",
    "#         # Select the next rows (if available)\n",
    "#         start_idx = 0 if len(selected_indices) == 0 else selected_indices[-1] + 1\n",
    "#         end_idx = start_idx + rows_per_iteration\n",
    "#         random_indices = np.arange(start_idx, min(end_idx, len(df_all1)))\n",
    "#     # Append the selected rows to the list\n",
    "#     selected_indices.extend(random_indices)\n",
    "#     # Create a DataFrame with the selected rows\n",
    "#     selected_df = df_all1.iloc[random_indices]\n",
    "#     # Append the seected DataFrame to the list\n",
    "#     selected_dfs1.append(selected_df)\n",
    "#     ind.append(selected_df.index)\n",
    "# # for i, selected_df in enumerate(selected_dfs):\n",
    "# #     print(f\"DataFrame {i+1}:\")\n",
    "# #     print(selected_df)\n",
    "# add = selected_dfs1[0].sample(n=len(selected_dfs1[0]) - len(selected_dfs1[1]), random_state=42)\n",
    "# selected_dfs1[1] = pd.concat([selected_dfs1[1], add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_dfs0 = []\n",
    "# rows_per_label = least_mod_num  \n",
    "# num_iterations = 5 \n",
    "# for _ in range(num_iterations):\n",
    "#     # Create an empty DataFrame with the same columns as df0_train\n",
    "#     selected_df = pd.DataFrame(columns=df_all0.columns)\n",
    "#     # Iterate through each unique label\n",
    "#     for label in df_all0['model'].unique():\n",
    "#         # Get rows with the current label\n",
    "#         label_rows = df_all0[df_all0['model'] == label]\n",
    "#         # If the label has fewer than 30 rows, randomly select rows to reach 30\n",
    "#         if len(label_rows) < rows_per_label:\n",
    "#             selected_rows = label_rows.sample(n=rows_per_label, replace=True)\n",
    "#         else:\n",
    "#             selected_rows = label_rows.sample(n=rows_per_label, replace=False)\n",
    "#         # Concatenate the selected rows to the selected_df\n",
    "#         selected_df = pd.concat([selected_df, selected_rows])\n",
    "#     # Append the selected DataFrame to the list\n",
    "#     selected_dfs0.append(selected_df)\n",
    "# # Print the selected DataFrames for each iteration\n",
    "# # for i, selected_df in enumerate(selected_dfs0):\n",
    "# #     print(f\"DataFrame {i+1}:\")\n",
    "# #     print(selected_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "8.0    6240\n",
       "2.0     780\n",
       "1.0     780\n",
       "6.0     780\n",
       "4.0     780\n",
       "3.0     780\n",
       "7.0     780\n",
       "5.0     780\n",
       "0.0     780\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[0]['model'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. CountV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer()\n",
    "# df_text_vec = vectorizer.fit_transform([' '.join(str(word) for word in sentence) for sentence in df['text']]).toarray()\n",
    "# dom2_text_vec = vectorizer.fit_transform([' '.join(str(word) for word in sentence) for sentence in dom2_even['text']]).toarray()\n",
    "# Test_text_vec = vectorizer.fit_transform([' '.join(str(word) for word in sentence) for sentence in Test_set['text']]).toarray()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse_matrix(sequences, vocab_size):\n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    data = []\n",
    "    for seq in sequences:\n",
    "        feature_counter = {}\n",
    "        for index in seq:\n",
    "            if index != 0:  # Skip 0s, other words\n",
    "                if index not in feature_counter:\n",
    "                    feature_counter[index] = 1\n",
    "                else:\n",
    "                    feature_counter[index] += 1\n",
    "        indices.extend(feature_counter.keys())\n",
    "        data.extend(feature_counter.values())\n",
    "        indptr.append(len(indices))\n",
    "    return csr_matrix((data, indices, indptr), dtype=int, shape=(len(sequences), vocab_size + 1))\n",
    "vocab_size = 4999\n",
    "\n",
    "Test_text_vec = to_sparse_matrix(Test_set['text'], vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_vec = to_sparse_matrix(df['text'], vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_vec_all = []\n",
    "y_df_all = []\n",
    "for i in range(len(df_all)):\n",
    "    df_text_vec_all.append(to_sparse_matrix(df_all[i]['text'], vocab_size))\n",
    "    y_df_all.append(df_all[i]['label'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_length'] = df['text'].apply(lambda x: len(x))\n",
    "Test_set['text_length'] = Test_set['text'].apply(lambda x: len(x))\n",
    "\n",
    "minimax_scale = MinMaxScaler()\n",
    "df['text_length'] = minimax_scale.fit_transform(df[['text_length']])\n",
    "Test_set['text_length'] = minimax_scale.fit_transform(Test_set[['text_length']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. for CountVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length_df = pd.DataFrame(df['text_length'])\n",
    "# X_df = np.concatenate([length_df, df_text_vec], axis=1)\n",
    "# y_df = df['label']\n",
    "# dom2_length_df = pd.DataFrame(dom2_even['text_length'])\n",
    "# X_dom2 = np.concatenate([dom2_length_df, dom2_text_vec], axis=1)\n",
    "# y_dom2 = dom2['label']\n",
    "# Test_length_df = pd.DataFrame(Test_set['text_length'])\n",
    "# X_Test = np.concatenate([Test_length_df, Test_text_vec], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. for sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = pd.DataFrame(df['text_length'])\n",
    "Test_length_df = pd.DataFrame(Test_set['text_length'])\n",
    "X_df = hstack([length_df, df_text_vec], format='csr')\n",
    "X_Test = hstack([Test_length_df, Test_text_vec], format='csr')\n",
    "# concatenated_df = pd.DataFrame(concatenated_data.toarray())\n",
    "y_df = df['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for sparse df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_df = []\n",
    "X_df_all = []\n",
    "for i in range(len(df_all)):\n",
    "    length_df.append(pd.DataFrame(df_all[i]['text_length']))\n",
    "    X_df_all.append(hstack([length_df[i], df_text_vec_all[i]], format='csr'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Kbest for Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [0 0 0 0 0] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y_df, test_size=0.2, random_state=42)\n",
    "k_values = []\n",
    "accuracies = []\n",
    "\n",
    "for k in range(100, (X_train.shape[1] + 1),100):\n",
    "    # Apply SelectKBest class to extract top k best features\n",
    "    best_features = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_train_kbest = best_features.fit_transform(X_train, y_train)\n",
    "    X_test_kbest = best_features.transform(X_test)\n",
    "    # train classifier\n",
    "    classifier = SVC(C=10, gamma=0.001)\n",
    "    classifier.fit(X_train_kbest, y_train)\n",
    "    # Predict\n",
    "    y_pred = classifier.predict(X_test_kbest)\n",
    "    acc = f1_score(y_test, y_pred)\n",
    "    k_values.append(k)\n",
    "    accuracies.append(acc)\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(k_values, accuracies, marker='o', linestyle='-')\n",
    "plt.xlabel('Number of Features (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('k vs Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=5001)\n",
    "X_new = selector.fit_transform(X_df, y_df)\n",
    "\n",
    "X_Test_new = selector.transform(X_Test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split for data all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = []\n",
    "X_test_list = []\n",
    "y_train_list = []\n",
    "y_test_list = []\n",
    "\n",
    "# Loop through each dataset and perform train-test splitting\n",
    "for i in range(len(df_all)):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_df_all[i], y_df_all[i], test_size=0.2, random_state=42\n",
    "    )\n",
    "    X_train_list.append(X_train)\n",
    "    X_test_list.append(X_test)\n",
    "    y_train_list.append(y_train)\n",
    "    y_test_list.append(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8737394957983193\n",
      "Test f1: 0.8765153071707419\n"
     ]
    }
   ],
   "source": [
    "model_svm = svm.SVC(C=10, gamma=0.001).fit(X_train, y_train)\n",
    "# Evaluation\n",
    "# y_pred_train = model_svm.predict(X_train)\n",
    "# print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "# y_pred_train = model_svm.predict(X_train)\n",
    "# print(\"Train f1:\", f1_score(y_train, y_pred_train))\n",
    "y_pred_test = model_svm.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "y_pred_test = model_svm.predict(X_test)\n",
    "print(\"Test f1:\", f1_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12480x5001 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 561339 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    512\n",
       "1    488\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pre = model_svm.predict(X_Test_new)\n",
    "result = pd.DataFrame({\n",
    "    'class': test_pre,\n",
    "})\n",
    "result['id'] = result.index\n",
    "result = result[['id'] + [col for col in result.columns if col != 'id']]\n",
    "result['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result_svm.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8449579831932773\n",
      "Test f1: 0.8482106129164952\n"
     ]
    }
   ],
   "source": [
    "model_log = LogisticRegression(solver='liblinear',multi_class='auto', penalty=\"l2\", C=1, max_iter=100).fit(X_train, y_train)\n",
    "# Evaluation\n",
    "# y_pred_train = model_log.predict(X_train)\n",
    "# print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "# y_pred_train = model_log.predict(X_train)\n",
    "# print(\"Train f1:\", f1_score(y_train, y_pred_train))\n",
    "y_pred_test = model_log.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "y_pred_test = model_log.predict(X_test)\n",
    "print(\"Test f1:\", f1_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    550\n",
       "0    450\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pre = model_log.predict(X_Test_new)\n",
    "result = pd.DataFrame({\n",
    "    'class': test_pre,\n",
    "})\n",
    "result['id'] = result.index\n",
    "result = result[['id'] + [col for col in result.columns if col != 'id']]\n",
    "result['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result_logistic.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8418067226890756\n",
      "Test f1: 0.839479855041569\n"
     ]
    }
   ],
   "source": [
    "model_mplc = MLPClassifier(activation= 'relu',solver='adam', alpha=0.0001, hidden_layer_sizes=(20,), random_state=1,\n",
    "                      learning_rate='adaptive').fit(X_train, y_train)\n",
    "# Evaluation\n",
    "# y_pred_train = model_mplc.predict(X_train)\n",
    "# print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "# y_pred_train = model_mplc.predict(X_train)\n",
    "# print(\"Train f1:\", f1_score(y_train, y_pred_train))\n",
    "y_pred_test = model_mplc.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "y_pred_test = model_mplc.predict(X_test)\n",
    "print(\"Test f1:\", f1_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "1    523\n",
       "0    477\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pre = model_mplc.predict(X_Test_new)\n",
    "result = pd.DataFrame({\n",
    "    'class': test_pre,\n",
    "})\n",
    "result['id'] = result.index\n",
    "result = result[['id'] + [col for col in result.columns if col != 'id']]\n",
    "result['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result_mlpc.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stack by data all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = {\n",
    "        ('gnb', GaussianNB(var_smoothing = 1.873817422860383e-06).fit(X_train_list[0].toarray(), y_train_list[0])),\n",
    "       \n",
    "        ('rf',RandomForestClassifier(n_estimators=800, criterion='entropy', min_samples_split=5, min_samples_leaf=1,\n",
    "                                  max_features='sqrt',max_depth= 100,bootstrap= False).fit(X_train_list[1], y_train_list[1])),\n",
    "        \n",
    "        ('svm', svm.SVC(C=20, gamma=0.0001).fit(X_train_list[2], y_train_list[2])),\n",
    "        \n",
    "        #('knn', KNeighborsClassifier(n_neighbors = 9, metric = 'minkowski', p = 2).fit(X_train_list[3], y_train_list[3])),\n",
    "        \n",
    "        ('mlpc', MLPClassifier(activation= 'relu',solver='adam', alpha=0.0001, hidden_layer_sizes=(20,), random_state=1,\n",
    "                      learning_rate='adaptive').fit(X_train_list[3], y_train_list[3]))\n",
    "    }\n",
    "stack_para = StackingClassifier(\n",
    "        estimators=estimators, final_estimator=LogisticRegression(solver='liblinear',multi_class='auto', penalty=\"l2\", C=0.5, max_iter=100))\n",
    "stack = stack_para.fit(X_train_list[4].toarray(), y_train_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9997996794871795\n",
      "Train f1: 0.9997999199679871\n",
      "Test Accuracy: 0.8918269230769231\n",
      "Test f1: 0.8831168831168831\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "y_pred_train = stack.predict(X_train.toarray())\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n",
    "y_pred_train = stack.predict(X_train.toarray())\n",
    "print(\"Train f1:\", f1_score(y_train, y_pred_train))\n",
    "y_pred_test = stack.predict(X_test.toarray())\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred_test))\n",
    "y_pred_test = stack.predict(X_test.toarray())\n",
    "print(\"Test f1:\", f1_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    698\n",
       "1    302\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pre = stack.predict(X_Test.toarray())\n",
    "result = pd.DataFrame({\n",
    "    'class': test_pre,\n",
    "})\n",
    "result['id'] = result.index\n",
    "result = result[['id'] + [col for col in result.columns if col != 'id']]\n",
    "result['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('result_stack.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
