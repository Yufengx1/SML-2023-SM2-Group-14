{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,  chi2\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom1 = []\n",
    "dom2 = []\n",
    "test_set = []\n",
    "with open('data/domain1_train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dom1.append(json.loads(line))\n",
    "        \n",
    "with open('data/domain2_train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dom2.append(json.loads(line))\n",
    "        \n",
    "sam = pd.read_csv(\"data/sample.csv\")\n",
    "\n",
    "with open('data/test_set.json', 'r') as file:\n",
    "    for line in file:\n",
    "        test_set.append(json.loads(line))\n",
    "        \n",
    "dom1 = pd.DataFrame.from_dict(dom1)\n",
    "dom2 = pd.DataFrame.from_dict(dom2)\n",
    "dom2 = dom2[dom2['text'].apply(len) > 0]\n",
    "test_set = pd.DataFrame.from_dict(test_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19500\n",
      "14899\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(dom1))\n",
    "print(len(dom2))\n",
    "print(len(sam))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12750\n",
      "2149\n"
     ]
    }
   ],
   "source": [
    "dom1['label'].value_counts()\n",
    "print(len(dom2.loc[dom2['label']==0]))\n",
    "print(len(dom2.loc[dom2['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([dom1, dom2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data['model'].fillna(8, inplace=True)\n",
    "\n",
    "\n",
    "X = data[['text','model']] \n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=data[['model', 'label']], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9936     0\n",
       "25220    0\n",
       "24497    0\n",
       "9566     1\n",
       "18884    0\n",
       "        ..\n",
       "29451    0\n",
       "4594     1\n",
       "29348    0\n",
       "26916    0\n",
       "28576    0\n",
       "Name: label, Length: 27519, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.654094\n",
      "1    0.345906\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "0    0.65407\n",
      "1    0.34593\n",
      "Name: proportion, dtype: float64\n",
      "model\n",
      "8.0    0.629347\n",
      "0.0    0.068716\n",
      "1.0    0.068534\n",
      "3.0    0.068534\n",
      "2.0    0.067989\n",
      "6.0    0.051274\n",
      "4.0    0.022930\n",
      "5.0    0.022675\n",
      "Name: proportion, dtype: float64\n",
      "model\n",
      "8.0    0.629360\n",
      "0.0    0.068750\n",
      "3.0    0.068605\n",
      "1.0    0.068459\n",
      "2.0    0.068023\n",
      "6.0    0.051163\n",
      "4.0    0.022965\n",
      "5.0    0.022674\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "print(X_train['model'].value_counts(normalize=True))\n",
    "print(X_test['model'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = X_train['text'].tolist()\n",
    "X1 =X_test['text'].tolist()\n",
    "max_length = max([len(seq) for seq in X])\n",
    "max_length1 = max([len(seq) for seq in X1])\n",
    "word2vec = Word2Vec(X, vector_size=300, window=10, min_count=1, workers=4)\n",
    "word2vec1 = Word2Vec(X1, vector_size=300, window=10, min_count=1, workers=4)\n",
    "\n",
    "def get_text_vector(text, word2vec_model):\n",
    "    vector_list = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv.index_to_key]\n",
    "    if len(vector_list) == 0:\n",
    "        return np.zeros(word2vec.vector_size)\n",
    "    return np.mean(vector_list, axis=0)\n",
    "\n",
    "X_train = np.array([get_text_vector(text, word2vec) for text in X])\n",
    "X_test = np.array([get_text_vector(text, word2vec1) for text in X1])\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.7)\n",
    "tomek = TomekLinks(sampling_strategy='majority')\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[('o', smote), ('u', tomek)])\n",
    "\n",
    "\n",
    "X_train, y_train = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.58554\n",
      "1    0.41446\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "30396    1\n",
       "30397    1\n",
       "30398    1\n",
       "30399    1\n",
       "30400    1\n",
       "Name: label, Length: 30401, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30401\n",
      "30401\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "batch_size = 64  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2744,  0.0799, -0.4938,  ..., -0.1175,  0.0989, -0.1265],\n",
       "        [-0.2202,  0.3285, -0.1467,  ..., -0.1497, -0.1044,  0.0025],\n",
       "        [-0.1474,  0.2807, -0.0995,  ..., -0.3048,  0.0586,  0.0586],\n",
       "        ...,\n",
       "        [-0.0128,  0.1453, -0.2720,  ..., -0.1786,  0.0658,  0.1536],\n",
       "        [-0.0091,  0.5468, -0.1467,  ..., -0.1265, -0.1568,  0.1994],\n",
       "        [-0.0825,  0.4063, -0.1860,  ..., -0.3794, -0.1103, -0.0289]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30401, 300])\n",
      "torch.Size([30401])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64  \n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 150, output_dim) # Assuming the length of sequences is 300, after pooling it will be 150\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Change the shape to (batch_size, input_dim, seq_len)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.size(0), -1)  # Flatten the tensor using reshape instead of view\n",
    "        x = self.fc1(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "\n",
    "input_dim = 300\n",
    "output_dim = 1\n",
    "\n",
    "model = CNNModel(input_dim, output_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 300])\n"
     ]
    }
   ],
   "source": [
    "# 假设 train_loader 和 test_loader 已被定义\n",
    "for batch in train_loader:\n",
    "    text, labels = batch\n",
    "    print(text.size())  # 这应该给你类似 (batch_size, seq_len, input_dim) 的输出\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.011\n",
      "\tTest Loss: 0.012\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.011\n",
      "\tTest Loss: 0.012\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.011\n",
      "\tTest Loss: 0.012\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.011\n",
      "\tTest Loss: 0.012\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.011\n",
      "\tTest Loss: 0.012\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.011\n",
      "\tTest Loss: 0.012\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.011\n",
      "\tTest Loss: 0.012\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.011\n",
      "\tTest Loss: 0.012\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.011\n",
      "\tTest Loss: 0.012\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.011\n",
      "\tTest Loss: 0.012\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        text = text.unsqueeze(2)  # Adding a channel dimension\n",
    "\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = criterion(predictions, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_samples += labels.size(0) \n",
    "\n",
    "    return total_loss / total_samples  \n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch  \n",
    "            text = text.unsqueeze(2)  # Adding a channel dimension\n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, labels.float())\n",
    "            total_loss += loss.item()\n",
    "            total_samples += labels.size(0) \n",
    "\n",
    "    return total_loss / total_samples  \n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss = evaluate(model, test_loader, criterion)\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
    "    best_test_loss = float('inf')\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNModel(\n",
      "  (conv1): Conv1d(1, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=19200, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('conv1.weight', tensor([[[ 0.3129,  0.4963, -0.3704]],\n",
      "\n",
      "        [[-0.2359, -0.0741, -0.5163]],\n",
      "\n",
      "        [[ 0.3737,  0.0266, -0.0522]],\n",
      "\n",
      "        [[-0.0656,  0.2748,  0.0875]],\n",
      "\n",
      "        [[-0.4687, -0.3957, -0.0183]],\n",
      "\n",
      "        [[-0.3248, -0.4346, -0.5567]],\n",
      "\n",
      "        [[-0.1224, -0.4323,  0.1989]],\n",
      "\n",
      "        [[ 0.5625, -0.5348, -0.3639]],\n",
      "\n",
      "        [[ 0.4276, -0.3076, -0.4434]],\n",
      "\n",
      "        [[ 0.1432,  0.1441,  0.3495]],\n",
      "\n",
      "        [[-0.3279, -0.0319,  0.1598]],\n",
      "\n",
      "        [[ 0.0876, -0.3682,  0.2386]],\n",
      "\n",
      "        [[-0.2241, -0.0266, -0.3116]],\n",
      "\n",
      "        [[-0.2189,  0.0779,  0.3427]],\n",
      "\n",
      "        [[-0.1317, -0.3078, -0.2342]],\n",
      "\n",
      "        [[-0.4304, -0.0047,  0.5203]],\n",
      "\n",
      "        [[ 0.4373, -0.5699, -0.2202]],\n",
      "\n",
      "        [[-0.0079,  0.0174, -0.4909]],\n",
      "\n",
      "        [[ 0.0458, -0.5333,  0.3216]],\n",
      "\n",
      "        [[-0.4678, -0.3653,  0.4644]],\n",
      "\n",
      "        [[-0.3523, -0.0725, -0.3164]],\n",
      "\n",
      "        [[-0.0502, -0.4994, -0.1163]],\n",
      "\n",
      "        [[-0.0274,  0.5268,  0.2163]],\n",
      "\n",
      "        [[ 0.2056, -0.2588, -0.3360]],\n",
      "\n",
      "        [[-0.2268,  0.4495, -0.3851]],\n",
      "\n",
      "        [[-0.0557,  0.5083, -0.0836]],\n",
      "\n",
      "        [[-0.3264,  0.2430,  0.1651]],\n",
      "\n",
      "        [[-0.0117, -0.3401, -0.0599]],\n",
      "\n",
      "        [[-0.1410, -0.3127, -0.4755]],\n",
      "\n",
      "        [[-0.0146, -0.4972, -0.3022]],\n",
      "\n",
      "        [[ 0.4550,  0.1823, -0.0237]],\n",
      "\n",
      "        [[-0.4591,  0.1370, -0.3346]],\n",
      "\n",
      "        [[ 0.2789,  0.4140, -0.2236]],\n",
      "\n",
      "        [[ 0.4930,  0.5596, -0.3760]],\n",
      "\n",
      "        [[-0.1262,  0.1369,  0.1038]],\n",
      "\n",
      "        [[-0.2826, -0.0448,  0.0851]],\n",
      "\n",
      "        [[-0.3969,  0.5718, -0.5134]],\n",
      "\n",
      "        [[ 0.0158,  0.4675, -0.4406]],\n",
      "\n",
      "        [[-0.5240, -0.2931, -0.0037]],\n",
      "\n",
      "        [[-0.5468, -0.4128,  0.5109]],\n",
      "\n",
      "        [[ 0.0605, -0.3954, -0.3326]],\n",
      "\n",
      "        [[ 0.3236, -0.2913,  0.3886]],\n",
      "\n",
      "        [[-0.3755, -0.2542,  0.0526]],\n",
      "\n",
      "        [[ 0.4208,  0.4555, -0.2854]],\n",
      "\n",
      "        [[ 0.3366,  0.1142, -0.5655]],\n",
      "\n",
      "        [[-0.4443, -0.0869,  0.5771]],\n",
      "\n",
      "        [[ 0.4497, -0.2063,  0.4967]],\n",
      "\n",
      "        [[ 0.1250, -0.5524, -0.2118]],\n",
      "\n",
      "        [[ 0.3014,  0.0795,  0.4949]],\n",
      "\n",
      "        [[ 0.0593,  0.4004, -0.0665]],\n",
      "\n",
      "        [[ 0.2985,  0.3864,  0.2700]],\n",
      "\n",
      "        [[ 0.2122, -0.3052, -0.0849]],\n",
      "\n",
      "        [[-0.5348, -0.3755,  0.0397]],\n",
      "\n",
      "        [[ 0.5165, -0.0297, -0.4648]],\n",
      "\n",
      "        [[ 0.5246, -0.3744,  0.0850]],\n",
      "\n",
      "        [[ 0.3465,  0.1509, -0.1669]],\n",
      "\n",
      "        [[ 0.0459, -0.4638, -0.1419]],\n",
      "\n",
      "        [[-0.2765,  0.2040, -0.4723]],\n",
      "\n",
      "        [[ 0.4380, -0.4315,  0.0696]],\n",
      "\n",
      "        [[-0.4082,  0.0203, -0.5210]],\n",
      "\n",
      "        [[ 0.3662, -0.1638,  0.0532]],\n",
      "\n",
      "        [[ 0.3354, -0.5213, -0.4372]],\n",
      "\n",
      "        [[ 0.0272, -0.4490,  0.4548]],\n",
      "\n",
      "        [[ 0.3810,  0.0151,  0.0348]],\n",
      "\n",
      "        [[-0.1496, -0.4586, -0.0370]],\n",
      "\n",
      "        [[ 0.1339, -0.3089, -0.2146]],\n",
      "\n",
      "        [[ 0.3750, -0.1174, -0.2588]],\n",
      "\n",
      "        [[ 0.3190,  0.2708,  0.1421]],\n",
      "\n",
      "        [[ 0.5052,  0.1111,  0.2004]],\n",
      "\n",
      "        [[-0.4053, -0.4544, -0.5309]],\n",
      "\n",
      "        [[-0.0194,  0.2550,  0.5705]],\n",
      "\n",
      "        [[-0.2141, -0.2428,  0.1924]],\n",
      "\n",
      "        [[ 0.5242,  0.3230, -0.4087]],\n",
      "\n",
      "        [[-0.2926, -0.2554, -0.0935]],\n",
      "\n",
      "        [[-0.4268, -0.4667,  0.4414]],\n",
      "\n",
      "        [[ 0.5711, -0.3080, -0.5653]],\n",
      "\n",
      "        [[ 0.2068,  0.0950,  0.2684]],\n",
      "\n",
      "        [[-0.3549,  0.2849,  0.3588]],\n",
      "\n",
      "        [[ 0.2747, -0.3299,  0.0202]],\n",
      "\n",
      "        [[ 0.0606, -0.0452, -0.1514]],\n",
      "\n",
      "        [[ 0.1173, -0.0988, -0.2082]],\n",
      "\n",
      "        [[ 0.4414,  0.5426,  0.3412]],\n",
      "\n",
      "        [[-0.0590,  0.2057, -0.2070]],\n",
      "\n",
      "        [[-0.5314, -0.3965, -0.4723]],\n",
      "\n",
      "        [[ 0.0543, -0.0601,  0.1656]],\n",
      "\n",
      "        [[ 0.4096, -0.2144,  0.4298]],\n",
      "\n",
      "        [[ 0.5538,  0.2912, -0.4665]],\n",
      "\n",
      "        [[-0.1372,  0.1678,  0.4366]],\n",
      "\n",
      "        [[-0.3727, -0.4832, -0.2275]],\n",
      "\n",
      "        [[-0.1278, -0.1121,  0.0425]],\n",
      "\n",
      "        [[ 0.0042, -0.5235, -0.2338]],\n",
      "\n",
      "        [[-0.3086, -0.3910,  0.2042]],\n",
      "\n",
      "        [[-0.4076,  0.5171,  0.1984]],\n",
      "\n",
      "        [[-0.1403, -0.0423, -0.4502]],\n",
      "\n",
      "        [[ 0.2043,  0.0651,  0.3239]],\n",
      "\n",
      "        [[-0.4365, -0.2621,  0.4413]],\n",
      "\n",
      "        [[-0.0722,  0.3542,  0.3052]],\n",
      "\n",
      "        [[-0.3178,  0.3342, -0.1929]],\n",
      "\n",
      "        [[-0.0674,  0.0263,  0.5745]],\n",
      "\n",
      "        [[-0.1221,  0.4024, -0.0231]],\n",
      "\n",
      "        [[-0.3592,  0.3355,  0.2348]],\n",
      "\n",
      "        [[ 0.3422,  0.1350, -0.2803]],\n",
      "\n",
      "        [[ 0.5067,  0.2831,  0.1051]],\n",
      "\n",
      "        [[ 0.3405,  0.1638, -0.0501]],\n",
      "\n",
      "        [[-0.4120,  0.5101,  0.1171]],\n",
      "\n",
      "        [[-0.1256, -0.3722,  0.2963]],\n",
      "\n",
      "        [[-0.0704,  0.5458,  0.1317]],\n",
      "\n",
      "        [[-0.5185,  0.5586,  0.1037]],\n",
      "\n",
      "        [[-0.1896, -0.1343, -0.1369]],\n",
      "\n",
      "        [[ 0.2901,  0.1311,  0.2150]],\n",
      "\n",
      "        [[-0.2384,  0.3635, -0.4557]],\n",
      "\n",
      "        [[ 0.4805, -0.5319, -0.0686]],\n",
      "\n",
      "        [[-0.1808,  0.3076, -0.3248]],\n",
      "\n",
      "        [[-0.2066,  0.4031, -0.0983]],\n",
      "\n",
      "        [[ 0.3975,  0.4733, -0.4166]],\n",
      "\n",
      "        [[-0.5377, -0.0895, -0.5741]],\n",
      "\n",
      "        [[ 0.0711,  0.3208, -0.2387]],\n",
      "\n",
      "        [[ 0.3963,  0.4729, -0.4388]],\n",
      "\n",
      "        [[ 0.5384, -0.5336,  0.0695]],\n",
      "\n",
      "        [[ 0.0083, -0.4653, -0.5682]],\n",
      "\n",
      "        [[-0.0221, -0.2659, -0.1432]],\n",
      "\n",
      "        [[ 0.3972, -0.3758, -0.1243]],\n",
      "\n",
      "        [[ 0.3303, -0.4665,  0.1702]],\n",
      "\n",
      "        [[-0.1370, -0.3881, -0.4453]],\n",
      "\n",
      "        [[ 0.2241, -0.3483,  0.0935]],\n",
      "\n",
      "        [[ 0.4325,  0.4096, -0.5108]],\n",
      "\n",
      "        [[ 0.1927, -0.3758, -0.2676]],\n",
      "\n",
      "        [[ 0.2812,  0.3053, -0.2778]]])), ('conv1.bias', tensor([ 1.5257e-01, -8.8302e-03, -2.4685e-01, -2.6619e-01, -4.7470e-02,\n",
      "        -5.3431e-01, -6.4249e-02,  5.2741e-01, -2.2603e-01, -4.1960e-01,\n",
      "         3.6844e-01,  9.9126e-03, -4.4155e-01, -3.8921e-01, -5.5550e-01,\n",
      "         7.1487e-02, -1.1669e-01, -5.0267e-01,  1.9534e-01, -3.1450e-01,\n",
      "         9.5373e-02,  4.4148e-01, -3.4398e-02,  5.3996e-02,  2.2556e-01,\n",
      "        -3.8277e-01, -3.3441e-01, -9.4019e-02,  1.3566e-04, -2.2147e-01,\n",
      "         5.4593e-01,  2.1905e-01, -3.8811e-01, -5.4677e-01, -5.6172e-01,\n",
      "        -5.7172e-01,  5.1277e-01, -5.2068e-01,  2.0975e-01, -4.7610e-01,\n",
      "         5.7076e-01, -3.8285e-01,  5.3714e-02, -4.1622e-01,  2.7199e-02,\n",
      "        -4.1484e-01,  5.4386e-01, -3.3805e-01, -2.0481e-01, -4.6882e-01,\n",
      "        -1.5677e-01,  2.9463e-01, -2.3872e-01, -3.7764e-01, -9.0716e-02,\n",
      "         4.1972e-01,  4.1225e-01, -5.5916e-03, -1.2226e-01, -2.1950e-01,\n",
      "        -1.6859e-01,  2.6582e-01,  1.3393e-01,  1.8416e-02,  4.3889e-02,\n",
      "         1.0674e-01,  3.8039e-01, -1.3554e-01,  3.7932e-01, -1.2061e-01,\n",
      "         2.1882e-01, -5.3411e-01, -1.6162e-03, -4.0948e-01, -5.1914e-01,\n",
      "         3.2918e-01, -3.2493e-01,  5.5471e-01,  5.4142e-01,  1.9234e-02,\n",
      "        -4.5662e-01, -5.6196e-01,  9.5506e-03,  1.8490e-01,  2.0221e-01,\n",
      "         4.0840e-01, -2.2983e-01, -3.7183e-02,  2.7719e-01,  2.6521e-01,\n",
      "        -3.8464e-01,  5.6507e-01, -4.2777e-01, -3.4946e-01, -3.6858e-02,\n",
      "         3.3281e-01, -1.3472e-01,  6.8033e-02,  1.0791e-02,  3.3963e-01,\n",
      "         2.5779e-01, -1.1700e-01, -3.8850e-01, -5.3484e-01,  2.3747e-01,\n",
      "         2.8011e-01, -2.9302e-01, -8.3526e-02,  5.3139e-01,  8.6426e-02,\n",
      "         3.2432e-01, -1.2715e-01, -4.1933e-01,  1.1075e-01,  2.2835e-01,\n",
      "        -4.3824e-01, -2.7840e-01,  5.3381e-01,  1.3932e-01,  4.2205e-01,\n",
      "        -5.5985e-01, -8.4252e-02, -5.1635e-01, -3.6776e-01, -2.1851e-02,\n",
      "         2.6075e-01, -4.8880e-01, -4.2692e-01])), ('fc1.weight', tensor([[-0.0006, -0.0034,  0.0046,  ...,  0.0001,  0.0005,  0.0010]])), ('fc1.bias', tensor([0.0045]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 =test_set['text'].tolist()\n",
    "max_length2 = max([len(seq) for seq in X2])\n",
    "word2vec2 = Word2Vec(X2, vector_size=300, window=10, min_count=1, workers=4)\n",
    "test_set = np.array([get_text_vector(text, word2vec2) for text in X2])\n",
    "test_set_tensor = torch.tensor(test_set, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.1950e-03,  3.8779e-01,  5.8890e-02,  ...,  2.4433e-02,\n",
       "          2.3045e-01, -1.9151e-01],\n",
       "        [-7.1910e-02,  2.7059e-01, -7.3820e-02,  ...,  7.1633e-03,\n",
       "          1.0798e-01, -2.3288e-01],\n",
       "        [ 2.2877e-02,  3.7363e-01,  7.3490e-02,  ...,  3.2050e-02,\n",
       "          1.7525e-01, -1.0136e-01],\n",
       "        ...,\n",
       "        [ 4.1560e-02,  3.9480e-01,  1.0509e-01,  ...,  3.4753e-02,\n",
       "          1.9286e-01, -8.0040e-02],\n",
       "        [ 2.0246e-02,  4.0686e-01,  7.6063e-02,  ...,  2.7383e-02,\n",
       "          2.4095e-01, -1.8053e-01],\n",
       "        [-2.4317e-02,  3.4818e-01,  2.7681e-04,  ...,  1.9132e-02,\n",
       "          1.7545e-01, -2.0126e-01]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "# 仅作示例，你需要使用你的数据和模型\n",
    "for data in test_set:  # Assuming train_loader is your training data loader\n",
    "    print(data.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Assuming test_set_tensor is a tensor containing your test data\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, text in enumerate(test_set_tensor):\n",
    "        try:\n",
    "            text = text.unsqueeze(0)\n",
    "            text = text.unsqueeze(0) # Adding a batch dimension\n",
    "            text = text.permute(0, 2, 1)  # Adjusting the order of dimensions\n",
    "            predictions_tensor = model(text)\n",
    "            predicted_class = (predictions_tensor > 0.575).float()\n",
    "            label = int(predicted_class.item())\n",
    "            predictions.append({\"id\": index, \"label\": label})\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing sample at index {index}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert predictions to a pandas DataFrame\n",
    "predictions_df = pd.DataFrame(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0, 'label': 0},\n",
       " {'id': 1, 'label': 1},\n",
       " {'id': 2, 'label': 0},\n",
       " {'id': 3, 'label': 0},\n",
       " {'id': 4, 'label': 0},\n",
       " {'id': 5, 'label': 0},\n",
       " {'id': 6, 'label': 0},\n",
       " {'id': 7, 'label': 0},\n",
       " {'id': 8, 'label': 1},\n",
       " {'id': 9, 'label': 0}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "output_csv_file = 'predictions.csv'\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'class']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for prediction in predictions:\n",
    "        writer.writerow({'id': prediction['id'], 'class': prediction['label']})\n",
    "\n",
    "print(f'Predictions saved to {output_csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>990</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>347 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  class\n",
       "1      1      1\n",
       "8      8      1\n",
       "10    10      1\n",
       "12    12      1\n",
       "13    13      1\n",
       "..   ...    ...\n",
       "987  987      1\n",
       "990  990      1\n",
       "993  993      1\n",
       "995  995      1\n",
       "999  999      1\n",
       "\n",
       "[347 rows x 2 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['class'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('cnn_word2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
