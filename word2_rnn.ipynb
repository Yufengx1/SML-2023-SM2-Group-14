{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,  chi2\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom1 = []\n",
    "dom2 = []\n",
    "test_set = []\n",
    "with open('data/domain1_train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dom1.append(json.loads(line))\n",
    "        \n",
    "with open('data/domain2_train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dom2.append(json.loads(line))\n",
    "        \n",
    "sam = pd.read_csv(\"data/sample.csv\")\n",
    "\n",
    "with open('data/test_set.json', 'r') as file:\n",
    "    for line in file:\n",
    "        test_set.append(json.loads(line))\n",
    "        \n",
    "dom1 = pd.DataFrame.from_dict(dom1)\n",
    "dom2 = pd.DataFrame.from_dict(dom2)\n",
    "dom2 = dom2[dom2['text'].apply(len) > 0]\n",
    "test_set = pd.DataFrame.from_dict(test_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19500\n",
      "14899\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(dom1))\n",
    "print(len(dom2))\n",
    "print(len(sam))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12750\n",
      "2149\n"
     ]
    }
   ],
   "source": [
    "dom1['label'].value_counts()\n",
    "print(len(dom2.loc[dom2['label']==0]))\n",
    "print(len(dom2.loc[dom2['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([dom1, dom2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data['model'].fillna(8, inplace=True)\n",
    "\n",
    "\n",
    "X = data[['text','model']] \n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=data[['model', 'label']], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9936     0\n",
       "25220    0\n",
       "24497    0\n",
       "9566     1\n",
       "18884    0\n",
       "        ..\n",
       "29451    0\n",
       "4594     1\n",
       "29348    0\n",
       "26916    0\n",
       "28576    0\n",
       "Name: label, Length: 27519, dtype: int64"
      ]
     },
     "execution_count": 1152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.654094\n",
      "1    0.345906\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "0    0.65407\n",
      "1    0.34593\n",
      "Name: proportion, dtype: float64\n",
      "model\n",
      "8.0    0.629347\n",
      "0.0    0.068716\n",
      "1.0    0.068534\n",
      "3.0    0.068534\n",
      "2.0    0.067989\n",
      "6.0    0.051274\n",
      "4.0    0.022930\n",
      "5.0    0.022675\n",
      "Name: proportion, dtype: float64\n",
      "model\n",
      "8.0    0.629360\n",
      "0.0    0.068750\n",
      "3.0    0.068605\n",
      "1.0    0.068459\n",
      "2.0    0.068023\n",
      "6.0    0.051163\n",
      "4.0    0.022965\n",
      "5.0    0.022674\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "print(X_train['model'].value_counts(normalize=True))\n",
    "print(X_test['model'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = X_train['text'].tolist()\n",
    "X1 =X_test['text'].tolist()\n",
    "max_length = max([len(seq) for seq in X])\n",
    "max_length1 = max([len(seq) for seq in X1])\n",
    "word2vec = Word2Vec(X, vector_size=300, window=10, min_count=1, workers=4)\n",
    "word2vec1 = Word2Vec(X1, vector_size=300, window=10, min_count=1, workers=4)\n",
    "\n",
    "def get_text_vector(text, word2vec_model):\n",
    "    vector_list = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv.index_to_key]\n",
    "    if len(vector_list) == 0:\n",
    "        return np.zeros(word2vec.vector_size)\n",
    "    return np.mean(vector_list, axis=0)\n",
    "\n",
    "X_train = np.array([get_text_vector(text, word2vec) for text in X])\n",
    "X_test = np.array([get_text_vector(text, word2vec1) for text in X1])\n",
    "\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.7)\n",
    "tomek = TomekLinks(sampling_strategy='majority')\n",
    "\n",
    "pipeline = Pipeline(steps=[('o', smote), ('u', tomek)])\n",
    "\n",
    "X_train, y_train = pipeline.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.585908\n",
      "1    0.414092\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "30423    1\n",
       "30424    1\n",
       "30425    1\n",
       "30426    1\n",
       "30427    1\n",
       "Name: label, Length: 30428, dtype: int64"
      ]
     },
     "execution_count": 1156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30428\n",
      "30428\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "batch_size = 64  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3184,  0.1813, -0.2902,  ..., -0.1018, -0.0891,  0.0371],\n",
       "        [-0.3175,  0.2098, -0.1292,  ...,  0.0512, -0.3820,  0.1591],\n",
       "        [-0.2872,  0.2351, -0.0929,  ..., -0.1217, -0.1570,  0.1076],\n",
       "        ...,\n",
       "        [-0.2398,  0.2525, -0.0054,  ..., -0.0459, -0.5334,  0.2688],\n",
       "        [-0.2144,  0.0092, -0.2064,  ...,  0.0857, -0.2025,  0.0526],\n",
       "        [-0.1500,  0.3700,  0.0022,  ..., -0.0974, -0.0920, -0.0395]])"
      ]
     },
     "execution_count": 1159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 1160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30428, 300])\n",
      "torch.Size([30428])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64  \n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        out = self.fc(hidden)\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import csv\n",
    "input_dim = 300  \n",
    "hidden_dim = 256 \n",
    "output_dim = 1 \n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.006\n",
      "\tTest Loss: 0.014\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.006\n",
      "\tTest Loss: 0.016\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.006\n",
      "\tTest Loss: 0.016\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.017\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.017\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.018\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.018\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.019\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.021\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.023\n"
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        text = text.unsqueeze(1)\n",
    "\n",
    "        predictions = model(text).squeeze(1)  \n",
    "        loss = criterion(predictions, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_samples += labels.size(0)  \n",
    "\n",
    "    return total_loss / total_samples  \n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch  \n",
    "            text = text.unsqueeze(1)      \n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, labels.float())\n",
    "            total_loss += loss.item()\n",
    "            total_samples += labels.size(0)  \n",
    "\n",
    "    return total_loss / total_samples  \n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss = evaluate(model, test_loader, criterion)\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
    "    best_test_loss = float('inf')\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'model_weights.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 =test_set['text'].tolist()\n",
    "max_length2 = max([len(seq) for seq in X2])\n",
    "word2vec2 = Word2Vec(X2, vector_size=300, window=10, min_count=1, workers=4)\n",
    "test_set = np.array([get_text_vector(text, word2vec2) for text in X2])\n",
    "test_set_tensor = torch.tensor(test_set, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0224,  0.4832,  0.1261,  ...,  0.1071,  0.2194, -0.1277],\n",
       "        [-0.0863,  0.3296, -0.0026,  ...,  0.0606,  0.1639, -0.2249],\n",
       "        [-0.0082,  0.4067,  0.1255,  ...,  0.0950,  0.1805, -0.0655],\n",
       "        ...,\n",
       "        [ 0.0097,  0.4410,  0.1632,  ...,  0.1038,  0.1873, -0.0308],\n",
       "        [-0.0147,  0.5067,  0.1441,  ...,  0.1138,  0.2276, -0.1135],\n",
       "        [-0.0473,  0.4123,  0.0657,  ...,  0.0882,  0.1994, -0.1681]])"
      ]
     },
     "execution_count": 1169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()\n",
    "predictions = []  \n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, text in enumerate(test_set_tensor):\n",
    "        text = text.view(-1, 1, 300)\n",
    "        predictions_tensor = model(text)\n",
    "        predicted_class = (predictions_tensor > 0.901).float() \n",
    "        label = int(predicted_class.item())         \n",
    "        predictions.append({\"id\": index, \"label\": label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "model.eval()\n",
    "values = []  \n",
    "with torch.no_grad():\n",
    "    for index, text in enumerate(test_set_tensor):\n",
    "        text = text.view(-1, 1, 300)\n",
    "        predictions_tensor = model(text)\n",
    "        values.append(predictions_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: 0.8506362438201904\n"
     ]
    }
   ],
   "source": [
    "tensor_list = [tensor.item() for tensor in values]\n",
    "numpy_array = np.array(tensor_list)\n",
    "\n",
    "median = np.median(numpy_array)\n",
    "\n",
    "print(\"median:\", median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "output_csv_file = 'predictions.csv'\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'class']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for prediction in predictions:\n",
    "        writer.writerow({'id': prediction['id'], 'class': prediction['label']})\n",
    "\n",
    "print(f'Predictions saved to {output_csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('rnn_word2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
