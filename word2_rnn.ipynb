{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,  chi2\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom1 = []\n",
    "dom2 = []\n",
    "test_set = []\n",
    "with open('data/domain1_train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dom1.append(json.loads(line))\n",
    "        \n",
    "with open('data/domain2_train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dom2.append(json.loads(line))\n",
    "        \n",
    "sam = pd.read_csv(\"data/sample.csv\")\n",
    "\n",
    "with open('data/test_set.json', 'r') as file:\n",
    "    for line in file:\n",
    "        test_set.append(json.loads(line))\n",
    "        \n",
    "dom1 = pd.DataFrame.from_dict(dom1)\n",
    "dom2 = pd.DataFrame.from_dict(dom2)\n",
    "dom2 = dom2[dom2['text'].apply(len) > 0]\n",
    "test_set = pd.DataFrame.from_dict(test_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19500\n",
      "14899\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(dom1))\n",
    "print(len(dom2))\n",
    "print(len(sam))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12750\n",
      "2149\n"
     ]
    }
   ],
   "source": [
    "dom1['label'].value_counts()\n",
    "print(len(dom2.loc[dom2['label']==0]))\n",
    "print(len(dom2.loc[dom2['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([dom1, dom2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data['model'].fillna(8, inplace=True)\n",
    "\n",
    "\n",
    "X = data[['text','model']] \n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=data[['model', 'label']], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9936     0\n",
       "25220    0\n",
       "24497    0\n",
       "9566     1\n",
       "18884    0\n",
       "        ..\n",
       "29451    0\n",
       "4594     1\n",
       "29348    0\n",
       "26916    0\n",
       "28576    0\n",
       "Name: label, Length: 27519, dtype: int64"
      ]
     },
     "execution_count": 1152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.654094\n",
      "1    0.345906\n",
      "Name: proportion, dtype: float64\n",
      "label\n",
      "0    0.65407\n",
      "1    0.34593\n",
      "Name: proportion, dtype: float64\n",
      "model\n",
      "8.0    0.629347\n",
      "0.0    0.068716\n",
      "1.0    0.068534\n",
      "3.0    0.068534\n",
      "2.0    0.067989\n",
      "6.0    0.051274\n",
      "4.0    0.022930\n",
      "5.0    0.022675\n",
      "Name: proportion, dtype: float64\n",
      "model\n",
      "8.0    0.629360\n",
      "0.0    0.068750\n",
      "3.0    0.068605\n",
      "1.0    0.068459\n",
      "2.0    0.068023\n",
      "6.0    0.051163\n",
      "4.0    0.022965\n",
      "5.0    0.022674\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "print(X_train['model'].value_counts(normalize=True))\n",
    "print(X_test['model'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = X_train['text'].tolist()\n",
    "X1 =X_test['text'].tolist()\n",
    "max_length = max([len(seq) for seq in X])\n",
    "max_length1 = max([len(seq) for seq in X1])\n",
    "word2vec = Word2Vec(X, vector_size=300, window=10, min_count=1, workers=4)\n",
    "word2vec1 = Word2Vec(X1, vector_size=300, window=10, min_count=1, workers=4)\n",
    "\n",
    "def get_text_vector(text, word2vec_model):\n",
    "    vector_list = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv.index_to_key]\n",
    "    if len(vector_list) == 0:\n",
    "        return np.zeros(word2vec.vector_size)\n",
    "    return np.mean(vector_list, axis=0)\n",
    "\n",
    "X_train = np.array([get_text_vector(text, word2vec) for text in X])\n",
    "X_test = np.array([get_text_vector(text, word2vec1) for text in X1])\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "smote = SMOTE(sampling_strategy=0.7)\n",
    "tomek = TomekLinks(sampling_strategy='majority')\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[('o', smote), ('u', tomek)])\n",
    "\n",
    "\n",
    "X_train, y_train = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    0.585908\n",
      "1    0.414092\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        0\n",
       "        ..\n",
       "30423    1\n",
       "30424    1\n",
       "30425    1\n",
       "30426    1\n",
       "30427    1\n",
       "Name: label, Length: 30428, dtype: int64"
      ]
     },
     "execution_count": 1156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30428\n",
      "30428\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "batch_size = 64  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3184,  0.1813, -0.2902,  ..., -0.1018, -0.0891,  0.0371],\n",
       "        [-0.3175,  0.2098, -0.1292,  ...,  0.0512, -0.3820,  0.1591],\n",
       "        [-0.2872,  0.2351, -0.0929,  ..., -0.1217, -0.1570,  0.1076],\n",
       "        ...,\n",
       "        [-0.2398,  0.2525, -0.0054,  ..., -0.0459, -0.5334,  0.2688],\n",
       "        [-0.2144,  0.0092, -0.2064,  ...,  0.0857, -0.2025,  0.0526],\n",
       "        [-0.1500,  0.3700,  0.0022,  ..., -0.0974, -0.0920, -0.0395]])"
      ]
     },
     "execution_count": 1159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 1, 1, 1])"
      ]
     },
     "execution_count": 1160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30428, 300])\n",
      "torch.Size([30428])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 64  \n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, num_layers=2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.lstm(x)\n",
    "        hidden = self.dropout(hidden[-1])\n",
    "        out = self.fc(hidden)\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import csv\n",
    "input_dim = 300  # 你的Word2Vec向量维度\n",
    "hidden_dim = 256  # LSTM隐藏层维度，可以根据需要调整\n",
    "output_dim = 1  # 输出维度，因为是二进制分类，所以是1\n",
    "\n",
    "model = RNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.006\n",
      "\tTest Loss: 0.014\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.006\n",
      "\tTest Loss: 0.016\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.006\n",
      "\tTest Loss: 0.016\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.017\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.017\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.018\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.018\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.019\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.021\n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.005\n",
      "\tTest Loss: 0.023\n"
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for batch in iterator:\n",
    "        text, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        text = text.unsqueeze(1)\n",
    "\n",
    "        predictions = model(text).squeeze(1)  # 使用squeeze消除不必要的维度\n",
    "        loss = criterion(predictions, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_samples += labels.size(0)  # 计算批次中的样本总数\n",
    "\n",
    "    return total_loss / total_samples  # 使用总样本数来计算平均损失\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch  \n",
    "            text = text.unsqueeze(1)      \n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, labels.float())\n",
    "            total_loss += loss.item()\n",
    "            total_samples += labels.size(0)  # 计算批次中的样本总数\n",
    "\n",
    "    return total_loss / total_samples  # 使用总样本数来计算平均损失\n",
    "\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss = evaluate(model, test_loader, criterion)\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
    "    best_test_loss = float('inf')\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'model_weights.pth')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNModel(\n",
      "  (lstm): LSTM(300, 256, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('lstm.weight_ih_l0', tensor([[ 0.2453,  0.0603,  0.2121,  ...,  0.0303,  0.0552, -0.2619],\n",
      "        [ 0.1133, -0.0146,  0.0457,  ..., -0.1483, -0.0580, -0.2832],\n",
      "        [ 0.0525, -0.0652,  0.0851,  ...,  0.0359,  0.0412, -0.0184],\n",
      "        ...,\n",
      "        [ 0.2066, -0.1242,  0.2158,  ..., -0.1076,  0.0676, -0.2079],\n",
      "        [-0.0741,  0.1407,  0.1275,  ..., -0.1842,  0.0337,  0.1243],\n",
      "        [ 0.0140, -0.0838,  0.0657,  ...,  0.1187,  0.0636,  0.0277]])), ('lstm.weight_hh_l0', tensor([[-2.3338e-37,  5.3347e-38, -2.8043e-37,  ..., -6.5526e-37,\n",
      "          1.4102e-37,  4.4913e-37],\n",
      "        [-2.4571e-37,  5.4040e-37,  1.1579e-39,  ...,  5.2356e-38,\n",
      "         -6.0960e-37,  9.3410e-38],\n",
      "        [-1.5984e-37, -6.7281e-37, -3.6514e-37,  ...,  2.2764e-37,\n",
      "         -8.8775e-38,  3.6292e-38],\n",
      "        ...,\n",
      "        [-2.3577e-37, -5.2705e-37,  8.1974e-39,  ..., -3.4465e-37,\n",
      "          5.0054e-37,  2.5038e-37],\n",
      "        [ 5.0215e-38, -1.6185e-37, -1.3887e-37,  ...,  6.8961e-37,\n",
      "         -7.0607e-37,  6.2778e-37],\n",
      "        [ 2.6248e-38, -1.7869e-38, -5.6753e-37,  ..., -1.8263e-37,\n",
      "          2.4245e-37, -9.1107e-39]])), ('lstm.bias_ih_l0', tensor([-0.0113, -0.1313, -0.1021,  ..., -0.1585,  0.0323, -0.0951])), ('lstm.bias_hh_l0', tensor([ 0.0486, -0.0802, -0.1055,  ..., -0.0896,  0.0363, -0.2007])), ('lstm.weight_ih_l1', tensor([[-0.1643, -0.0692,  0.0978,  ..., -0.0552,  0.3193, -0.1090],\n",
      "        [-0.1443, -0.1172, -0.0160,  ..., -0.0779,  0.1470, -0.0727],\n",
      "        [ 0.0104, -0.2078, -0.0151,  ..., -0.2574,  0.1174, -0.0383],\n",
      "        ...,\n",
      "        [-0.0456, -0.0807, -0.0665,  ..., -0.1959, -0.0024, -0.1234],\n",
      "        [-0.1757, -0.2458,  0.0175,  ..., -0.1861,  0.3093, -0.0356],\n",
      "        [-0.0957, -0.2149,  0.0916,  ..., -0.2474,  0.1289,  0.0292]])), ('lstm.weight_hh_l1', tensor([[-1.5886e-37, -2.0572e-37,  3.2419e-37,  ..., -5.9245e-37,\n",
      "         -2.2869e-37, -9.0532e-38],\n",
      "        [-1.0982e-37,  3.0975e-37,  1.0767e-37,  ...,  2.5844e-37,\n",
      "          6.8256e-37, -6.3730e-38],\n",
      "        [-1.4806e-37, -2.5437e-37, -2.7750e-37,  ..., -3.8638e-38,\n",
      "          2.0217e-37, -2.8634e-37],\n",
      "        ...,\n",
      "        [-2.0988e-37, -2.6175e-39, -1.5144e-37,  ..., -3.9408e-37,\n",
      "         -5.9017e-37, -2.6518e-37],\n",
      "        [-3.7705e-37,  2.3980e-37,  9.0057e-39,  ...,  4.3210e-37,\n",
      "          1.1306e-37, -1.4314e-37],\n",
      "        [ 4.3785e-38,  2.2918e-37, -2.4767e-37,  ..., -3.9428e-37,\n",
      "         -2.2527e-37,  4.5247e-37]])), ('lstm.bias_ih_l1', tensor([0.1106, 0.0536, 0.0755,  ..., 0.0139, 0.0929, 0.1217])), ('lstm.bias_hh_l1', tensor([ 0.0946,  0.1005,  0.1157,  ..., -0.0132,  0.1024,  0.0535])), ('fc.weight', tensor([[ 0.1640, -0.1231, -0.1289,  0.1995, -0.1174, -0.1238, -0.1882, -0.1383,\n",
      "         -0.1061,  0.1177, -0.0983, -0.0852,  0.0785, -0.1059,  0.2288, -0.1221,\n",
      "         -0.0891,  0.1519,  0.0646, -0.0813, -0.0613, -0.1177, -0.1436,  0.1902,\n",
      "         -0.1421, -0.0456, -0.1181, -0.0717, -0.1195, -0.1612,  0.1666, -0.1127,\n",
      "          0.1089,  0.1858, -0.1373,  0.0896, -0.1227, -0.0319, -0.1417,  0.1394,\n",
      "         -0.1352, -0.1160, -0.1609, -0.1034, -0.1442, -0.1565, -0.1656, -0.1043,\n",
      "         -0.1812,  0.1201,  0.0671,  0.1408,  0.1115,  0.1059, -0.0089,  0.1783,\n",
      "          0.1415,  0.1408,  0.1504, -0.1221,  0.1023, -0.1308,  0.1568,  0.1146,\n",
      "         -0.1118,  0.0885,  0.1367,  0.1614,  0.1209,  0.1560,  0.1139, -0.1232,\n",
      "          0.0965, -0.0843, -0.1662,  0.2275,  0.1325, -0.1427, -0.2283, -0.1088,\n",
      "         -0.0845, -0.1331, -0.1311, -0.1165, -0.1521, -0.1585,  0.1932, -0.1348,\n",
      "         -0.1210, -0.0964,  0.1011, -0.1594,  0.1621,  0.1218, -0.0339,  0.1173,\n",
      "         -0.0684,  0.0958, -0.1107, -0.1928,  0.1171, -0.2043,  0.0912,  0.1093,\n",
      "         -0.1374, -0.1081, -0.0964,  0.1297,  0.1953, -0.1073, -0.1260, -0.1118,\n",
      "          0.0989, -0.1255, -0.1527,  0.1147, -0.1407,  0.1134, -0.1389, -0.1443,\n",
      "          0.1541, -0.1573,  0.0374,  0.0109,  0.1567, -0.1442, -0.0905, -0.0319,\n",
      "          0.1813, -0.0715,  0.0762, -0.0633, -0.0908,  0.1493, -0.1026,  0.1207,\n",
      "          0.1886,  0.1620,  0.1497,  0.0248, -0.2093, -0.1035, -0.1372,  0.1505,\n",
      "          0.1842,  0.0891, -0.1246,  0.1710,  0.1363,  0.0811,  0.1000,  0.1648,\n",
      "          0.0778, -0.0671, -0.1426,  0.1716, -0.1052, -0.1078,  0.1850,  0.0948,\n",
      "          0.1593, -0.0579,  0.1206,  0.1092,  0.1601, -0.0930, -0.1286,  0.1598,\n",
      "         -0.1329, -0.1024,  0.0949, -0.1203, -0.1492,  0.0332,  0.1167, -0.1284,\n",
      "          0.1273, -0.1269, -0.1476,  0.1276, -0.1557, -0.1590, -0.0965, -0.1547,\n",
      "         -0.1156,  0.1231,  0.1431,  0.1034,  0.0835, -0.1089,  0.1292,  0.0397,\n",
      "         -0.1465,  0.1626,  0.1244, -0.1306, -0.1353,  0.1497,  0.1128, -0.1264,\n",
      "          0.1030, -0.1343,  0.0709, -0.1491,  0.0902,  0.1269,  0.0843,  0.1932,\n",
      "         -0.1099,  0.1703, -0.1086,  0.1069,  0.1700,  0.0641, -0.1812,  0.1118,\n",
      "          0.1417, -0.0628,  0.1891, -0.0811, -0.0826, -0.1102, -0.0911,  0.1208,\n",
      "          0.0543,  0.1342,  0.1086, -0.1131, -0.1371, -0.1085, -0.0651,  0.0704,\n",
      "          0.1512,  0.1782,  0.2159,  0.1182,  0.1022, -0.1715, -0.1465,  0.0973,\n",
      "         -0.1242, -0.1204,  0.0864, -0.0568,  0.1170,  0.2008, -0.1093, -0.2078,\n",
      "          0.1069,  0.1604,  0.1358, -0.1658, -0.1889,  0.1049, -0.1440, -0.1315]])), ('fc.bias', tensor([0.0471]))])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 =test_set['text'].tolist()\n",
    "max_length2 = max([len(seq) for seq in X2])\n",
    "word2vec2 = Word2Vec(X2, vector_size=300, window=10, min_count=1, workers=4)\n",
    "test_set = np.array([get_text_vector(text, word2vec2) for text in X2])\n",
    "test_set_tensor = torch.tensor(test_set, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0224,  0.4832,  0.1261,  ...,  0.1071,  0.2194, -0.1277],\n",
       "        [-0.0863,  0.3296, -0.0026,  ...,  0.0606,  0.1639, -0.2249],\n",
       "        [-0.0082,  0.4067,  0.1255,  ...,  0.0950,  0.1805, -0.0655],\n",
       "        ...,\n",
       "        [ 0.0097,  0.4410,  0.1632,  ...,  0.1038,  0.1873, -0.0308],\n",
       "        [-0.0147,  0.5067,  0.1441,  ...,  0.1138,  0.2276, -0.1135],\n",
       "        [-0.0473,  0.4123,  0.0657,  ...,  0.0882,  0.1994, -0.1681]])"
      ]
     },
     "execution_count": 1169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 假设test_data是你的测试数据，是一个列表，每个元素都是数字化的句子\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 假设test_data是你的测试数据，是一个Pandas DataFrame，包含一个名为'text'的列\n",
    "predictions = []  # 用于存储预测结果的列表\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, text in enumerate(test_set_tensor):\n",
    "        text = text.view(-1, 1, 300)\n",
    "        predictions_tensor = model(text)\n",
    "        predicted_class = (predictions_tensor > 0.901).float() \n",
    "        label = int(predicted_class.item())         \n",
    "        predictions.append({\"id\": index, \"label\": label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('model_weights.pth'))\n",
    "\n",
    "# 设置模型为评估模式\n",
    "model.eval()\n",
    "\n",
    "# 假设test_data是你的测试数据，是一个列表，每个元素都是数字化的句子\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 假设test_data是你的测试数据，是一个Pandas DataFrame，包含一个名为'text'的列\n",
    "values = []  # 用于存储预测结果的列表\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index, text in enumerate(test_set_tensor):\n",
    "        text = text.view(-1, 1, 300)\n",
    "        predictions_tensor = model(text)\n",
    "        values.append(predictions_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "median: 0.8506362438201904\n"
     ]
    }
   ],
   "source": [
    "tensor_list = [tensor.item() for tensor in values]\n",
    "numpy_array = np.array(tensor_list)\n",
    "\n",
    "median = np.median(numpy_array)\n",
    "\n",
    "print(\"median:\", median)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0, 'label': 0},\n",
       " {'id': 1, 'label': 1},\n",
       " {'id': 2, 'label': 0},\n",
       " {'id': 3, 'label': 0},\n",
       " {'id': 4, 'label': 0},\n",
       " {'id': 5, 'label': 0},\n",
       " {'id': 6, 'label': 0},\n",
       " {'id': 7, 'label': 0},\n",
       " {'id': 8, 'label': 1},\n",
       " {'id': 9, 'label': 0}]"
      ]
     },
     "execution_count": 1173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "output_csv_file = 'predictions.csv'\n",
    "with open(output_csv_file, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['id', 'class']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    for prediction in predictions:\n",
    "        writer.writerow({'id': prediction['id'], 'class': prediction['label']})\n",
    "\n",
    "print(f'Predictions saved to {output_csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1175,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>983</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>986</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>987</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>990</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>266 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  class\n",
       "1      1      1\n",
       "8      8      1\n",
       "13    13      1\n",
       "14    14      1\n",
       "15    15      1\n",
       "..   ...    ...\n",
       "983  983      1\n",
       "986  986      1\n",
       "987  987      1\n",
       "990  990      1\n",
       "995  995      1\n",
       "\n",
       "[266 rows x 2 columns]"
      ]
     },
     "execution_count": 1176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[result['class'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1177,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('rnn_word2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
