{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif,  chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom1 = []\n",
    "dom2 = []\n",
    "test_set = []\n",
    "with open('data/domain1_train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dom1.append(json.loads(line))\n",
    "        \n",
    "with open('data/domain2_train.json', 'r') as file:\n",
    "    for line in file:\n",
    "        dom2.append(json.loads(line))\n",
    "        \n",
    "        \n",
    "with open('data/test_set.json', 'r') as file:\n",
    "    for line in file:\n",
    "        test_set.append(json.loads(line))\n",
    "        \n",
    "dom1 = pd.DataFrame.from_dict(dom1)\n",
    "dom2 = pd.DataFrame.from_dict(dom2)\n",
    "dom2 = dom2[dom2['text'].apply(len) > 0]\n",
    "kaggle_set = pd.DataFrame.from_dict(test_set)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19500\n",
      "14899\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(dom1))\n",
    "print(len(dom2))\n",
    "print(len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12750\n",
      "2149\n"
     ]
    }
   ],
   "source": [
    "dom1['label'].value_counts()\n",
    "print(len(dom2.loc[dom2['label']==0]))\n",
    "print(len(dom2.loc[dom2['label']==1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train model with dom2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11919, 3)\n",
      "(2980, 3)\n"
     ]
    }
   ],
   "source": [
    "d2_train,d2_test,y2_train,y2_test = train_test_split(dom2,dom2['label'],test_size=0.2)\n",
    "print(d2_train.shape)\n",
    "print(d2_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom2_80_percent_count = int(len(dom2) * 0.8)\n",
    "\n",
    "dom2_80_percent = dom2.iloc[:dom2_80_percent_count]\n",
    "\n",
    "merged_dataset = pd.concat([dom1, dom2_80_percent], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25135, 3)\n",
      "(6284, 3)\n"
     ]
    }
   ],
   "source": [
    "d1_train, d1_test,y1_train,y1_test = train_test_split(merged_dataset, merged_dataset['label'],test_size=0.2)\n",
    "print(d1_train.shape)\n",
    "print(d1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = d2_train['text'].tolist()\n",
    "X1 =d2_test['text'].tolist()\n",
    "max_length = max([len(seq) for seq in X])\n",
    "max_length1 = max([len(seq) for seq in X1])\n",
    "word2vec = Word2Vec(X, vector_size=300, window=10, min_count=1, workers=4)\n",
    "word2vec1 = Word2Vec(X1, vector_size=300, window=10, min_count=1, workers=4)\n",
    "\n",
    "def get_text_vector(text, word2vec_model):\n",
    "    vector_list = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv.index_to_key]\n",
    "    if len(vector_list) == 0:\n",
    "        return np.zeros(word2vec.vector_size)\n",
    "    return np.mean(vector_list, axis=0)\n",
    "\n",
    "X_train = np.array([get_text_vector(text, word2vec) for text in X])\n",
    "X_test = np.array([get_text_vector(text, word2vec1) for text in X1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X2 = d1_train['text'].tolist()\n",
    "X3 =d1_test['text'].tolist()\n",
    "max_length = max([len(seq) for seq in X2])\n",
    "max_length1 = max([len(seq) for seq in X3])\n",
    "word2vec = Word2Vec(X2, vector_size=300, window=10, min_count=1, workers=4)\n",
    "word2vec1 = Word2Vec(X3, vector_size=300, window=10, min_count=1, workers=4)\n",
    "\n",
    "def get_text_vector(text, word2vec_model):\n",
    "    vector_list = [word2vec_model.wv[word] for word in text if word in word2vec_model.wv.index_to_key]\n",
    "    if len(vector_list) == 0:\n",
    "        return np.zeros(word2vec.vector_size)\n",
    "    return np.mean(vector_list, axis=0)\n",
    "\n",
    "X_train1 = np.array([get_text_vector(text, word2vec) for text in X2])\n",
    "X_test1 = np.array([get_text_vector(text, word2vec1) for text in X3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11919, 300)"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((11919, 300, 1))\n",
    "X_test = X_test.reshape((2980, 300, 1))\n",
    "X_train1 = X_train1.reshape((25135, 300, 1))\n",
    "X_test1 = X_test1.reshape((6284, 300, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "187/187 [==============================] - 8s 33ms/step - loss: 0.8755 - accuracy: 0.2995 - f1_metric: 0.2497\n",
      "Epoch 2/100\n",
      "187/187 [==============================] - 6s 33ms/step - loss: 0.6884 - accuracy: 0.7731 - f1_metric: 0.2495\n",
      "Epoch 3/100\n",
      "187/187 [==============================] - 5s 27ms/step - loss: 0.6852 - accuracy: 0.8127 - f1_metric: 0.2503\n",
      "Epoch 4/100\n",
      "187/187 [==============================] - 4s 23ms/step - loss: 0.6931 - accuracy: 0.7296 - f1_metric: 0.2505\n",
      "Epoch 5/100\n",
      "187/187 [==============================] - 5s 24ms/step - loss: 0.6854 - accuracy: 0.8127 - f1_metric: 0.2507\n",
      "Epoch 6/100\n",
      "187/187 [==============================] - 5s 27ms/step - loss: 0.6936 - accuracy: 0.8116 - f1_metric: 0.2495\n",
      "Epoch 7/100\n",
      "187/187 [==============================] - 5s 27ms/step - loss: 0.7085 - accuracy: 0.7272 - f1_metric: 0.2495\n",
      "Epoch 8/100\n",
      "187/187 [==============================] - 7s 39ms/step - loss: 0.6947 - accuracy: 0.4006 - f1_metric: 0.2492\n",
      "Epoch 9/100\n",
      "187/187 [==============================] - 7s 35ms/step - loss: 0.6936 - accuracy: 0.5743 - f1_metric: 0.2498\n",
      "Epoch 10/100\n",
      "187/187 [==============================] - 8s 41ms/step - loss: 0.6931 - accuracy: 0.4775 - f1_metric: 0.2498\n",
      "Epoch 11/100\n",
      "187/187 [==============================] - 7s 39ms/step - loss: 0.6932 - accuracy: 0.4405 - f1_metric: 0.2508\n",
      "Epoch 12/100\n",
      "187/187 [==============================] - 6s 34ms/step - loss: 0.6933 - accuracy: 0.5025 - f1_metric: 0.2484\n",
      "Epoch 13/100\n",
      "187/187 [==============================] - 6s 30ms/step - loss: 0.6933 - accuracy: 0.7048 - f1_metric: 0.2504\n",
      "Epoch 14/100\n",
      "187/187 [==============================] - 7s 36ms/step - loss: 0.6933 - accuracy: 0.4006 - f1_metric: 0.2506\n",
      "Epoch 15/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6932 - accuracy: 0.5227 - f1_metric: 0.2504\n",
      "Epoch 16/100\n",
      "187/187 [==============================] - 6s 29ms/step - loss: 0.6936 - accuracy: 0.5126 - f1_metric: 0.2487\n",
      "Epoch 17/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.4573 - f1_metric: 0.2497\n",
      "Epoch 18/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.7243 - f1_metric: 0.2489\n",
      "Epoch 19/100\n",
      "187/187 [==============================] - 5s 27ms/step - loss: 0.6933 - accuracy: 0.3736 - f1_metric: 0.2495\n",
      "Epoch 20/100\n",
      "187/187 [==============================] - 5s 26ms/step - loss: 0.6933 - accuracy: 0.7475 - f1_metric: 0.2494\n",
      "Epoch 21/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6933 - accuracy: 0.4871 - f1_metric: 0.2506\n",
      "Epoch 22/100\n",
      "187/187 [==============================] - 6s 33ms/step - loss: 0.6932 - accuracy: 0.5176 - f1_metric: 0.2510\n",
      "Epoch 23/100\n",
      "187/187 [==============================] - 6s 34ms/step - loss: 0.6933 - accuracy: 0.2836 - f1_metric: 0.2495\n",
      "Epoch 24/100\n",
      "187/187 [==============================] - 5s 25ms/step - loss: 0.6933 - accuracy: 0.6292 - f1_metric: 0.2496\n",
      "Epoch 25/100\n",
      "187/187 [==============================] - 5s 26ms/step - loss: 0.6932 - accuracy: 0.7796 - f1_metric: 0.2500\n",
      "Epoch 26/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.3701 - f1_metric: 0.2497\n",
      "Epoch 27/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6933 - accuracy: 0.6041 - f1_metric: 0.2497\n",
      "Epoch 28/100\n",
      "187/187 [==============================] - 5s 25ms/step - loss: 0.6932 - accuracy: 0.4582 - f1_metric: 0.2497\n",
      "Epoch 29/100\n",
      "187/187 [==============================] - 4s 23ms/step - loss: 0.6932 - accuracy: 0.5672 - f1_metric: 0.2495\n",
      "Epoch 30/100\n",
      "187/187 [==============================] - 4s 22ms/step - loss: 0.6932 - accuracy: 0.4081 - f1_metric: 0.2489\n",
      "Epoch 31/100\n",
      "187/187 [==============================] - 4s 23ms/step - loss: 0.6933 - accuracy: 0.7199 - f1_metric: 0.2508\n",
      "Epoch 32/100\n",
      "187/187 [==============================] - 4s 23ms/step - loss: 0.6933 - accuracy: 0.2474 - f1_metric: 0.2503\n",
      "Epoch 33/100\n",
      "187/187 [==============================] - 5s 24ms/step - loss: 0.6931 - accuracy: 0.6605 - f1_metric: 0.2497\n",
      "Epoch 34/100\n",
      "187/187 [==============================] - 11s 57ms/step - loss: 0.6933 - accuracy: 0.6152 - f1_metric: 0.2509\n",
      "Epoch 35/100\n",
      "187/187 [==============================] - 8s 40ms/step - loss: 0.6933 - accuracy: 0.3857 - f1_metric: 0.2499\n",
      "Epoch 36/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6932 - accuracy: 0.4208 - f1_metric: 0.2492\n",
      "Epoch 37/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6932 - accuracy: 0.5476 - f1_metric: 0.2499\n",
      "Epoch 38/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6932 - accuracy: 0.3674 - f1_metric: 0.2492\n",
      "Epoch 39/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6933 - accuracy: 0.5048 - f1_metric: 0.2500\n",
      "Epoch 40/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6932 - accuracy: 0.7998 - f1_metric: 0.2495\n",
      "Epoch 41/100\n",
      "187/187 [==============================] - 6s 29ms/step - loss: 0.6933 - accuracy: 0.4793 - f1_metric: 0.2483\n",
      "Epoch 42/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.6501 - f1_metric: 0.2504\n",
      "Epoch 43/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6933 - accuracy: 0.6177 - f1_metric: 0.2494\n",
      "Epoch 44/100\n",
      "187/187 [==============================] - 6s 34ms/step - loss: 0.6933 - accuracy: 0.6719 - f1_metric: 0.2507\n",
      "Epoch 45/100\n",
      "187/187 [==============================] - 5s 25ms/step - loss: 0.6932 - accuracy: 0.2772 - f1_metric: 0.2494\n",
      "Epoch 46/100\n",
      "187/187 [==============================] - 5s 26ms/step - loss: 0.6932 - accuracy: 0.7695 - f1_metric: 0.2507\n",
      "Epoch 47/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.2213 - f1_metric: 0.2496\n",
      "Epoch 48/100\n",
      "187/187 [==============================] - 5s 27ms/step - loss: 0.6932 - accuracy: 0.5243 - f1_metric: 0.2497\n",
      "Epoch 49/100\n",
      "187/187 [==============================] - 6s 32ms/step - loss: 0.6933 - accuracy: 0.3768 - f1_metric: 0.2497\n",
      "Epoch 50/100\n",
      "187/187 [==============================] - 8s 45ms/step - loss: 0.6932 - accuracy: 0.6459 - f1_metric: 0.2507\n",
      "Epoch 51/100\n",
      "187/187 [==============================] - 6s 32ms/step - loss: 0.6932 - accuracy: 0.3064 - f1_metric: 0.2495\n",
      "Epoch 52/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6932 - accuracy: 0.6721 - f1_metric: 0.2506\n",
      "Epoch 53/100\n",
      "187/187 [==============================] - 6s 30ms/step - loss: 0.6932 - accuracy: 0.6103 - f1_metric: 0.2494\n",
      "Epoch 54/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.2759 - f1_metric: 0.2486\n",
      "Epoch 55/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.7562 - f1_metric: 0.2505\n",
      "Epoch 56/100\n",
      "187/187 [==============================] - 4s 24ms/step - loss: 0.6932 - accuracy: 0.3088 - f1_metric: 0.2503\n",
      "Epoch 57/100\n",
      "187/187 [==============================] - 4s 23ms/step - loss: 0.6932 - accuracy: 0.7183 - f1_metric: 0.2499\n",
      "Epoch 58/100\n",
      "187/187 [==============================] - 5s 25ms/step - loss: 0.6933 - accuracy: 0.1445 - f1_metric: 0.2492\n",
      "Epoch 59/100\n",
      "187/187 [==============================] - 7s 37ms/step - loss: 0.6932 - accuracy: 0.6544 - f1_metric: 0.2507\n",
      "Epoch 60/100\n",
      "187/187 [==============================] - 5s 26ms/step - loss: 0.6932 - accuracy: 0.1963 - f1_metric: 0.2504\n",
      "Epoch 61/100\n",
      "187/187 [==============================] - 6s 30ms/step - loss: 0.6932 - accuracy: 0.5437 - f1_metric: 0.2499\n",
      "Epoch 62/100\n",
      "187/187 [==============================] - 6s 33ms/step - loss: 0.6932 - accuracy: 0.3739 - f1_metric: 0.2489\n",
      "Epoch 63/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.4265 - f1_metric: 0.2492\n",
      "Epoch 64/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.3714 - f1_metric: 0.2498\n",
      "Epoch 65/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6933 - accuracy: 0.5931 - f1_metric: 0.2486\n",
      "Epoch 66/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.6629 - f1_metric: 0.2505\n",
      "Epoch 67/100\n",
      "187/187 [==============================] - 6s 30ms/step - loss: 0.6932 - accuracy: 0.6466 - f1_metric: 0.2493\n",
      "Epoch 68/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6932 - accuracy: 0.7242 - f1_metric: 0.2499\n",
      "Epoch 69/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.5191 - f1_metric: 0.2501\n",
      "Epoch 70/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.3301 - f1_metric: 0.2497\n",
      "Epoch 71/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.6192 - f1_metric: 0.2501\n",
      "Epoch 72/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.4791 - f1_metric: 0.2492\n",
      "Epoch 73/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.3998 - f1_metric: 0.2495\n",
      "Epoch 74/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.3016 - f1_metric: 0.2509\n",
      "Epoch 75/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.2998 - f1_metric: 0.2498\n",
      "Epoch 76/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.5483 - f1_metric: 0.2497\n",
      "Epoch 77/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.4810 - f1_metric: 0.2492\n",
      "Epoch 78/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.4926 - f1_metric: 0.2504\n",
      "Epoch 79/100\n",
      "187/187 [==============================] - 5s 27ms/step - loss: 0.6932 - accuracy: 0.5896 - f1_metric: 0.2497\n",
      "Epoch 80/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.5737 - f1_metric: 0.2501\n",
      "Epoch 81/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.2951 - f1_metric: 0.2497\n",
      "Epoch 82/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6933 - accuracy: 0.4543 - f1_metric: 0.2495\n",
      "Epoch 83/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.8555 - f1_metric: 0.2499\n",
      "Epoch 84/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.1445 - f1_metric: 0.2494\n",
      "Epoch 85/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.5812 - f1_metric: 0.2502\n",
      "Epoch 86/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.2872 - f1_metric: 0.2508\n",
      "Epoch 87/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.2900 - f1_metric: 0.2506\n",
      "Epoch 88/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.6705 - f1_metric: 0.2507\n",
      "Epoch 89/100\n",
      "187/187 [==============================] - 5s 28ms/step - loss: 0.6932 - accuracy: 0.1445 - f1_metric: 0.2509\n",
      "Epoch 90/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.5793 - f1_metric: 0.2505\n",
      "Epoch 91/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.6024 - f1_metric: 0.2500\n",
      "Epoch 92/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6932 - accuracy: 0.3823 - f1_metric: 0.2492\n",
      "Epoch 93/100\n",
      "187/187 [==============================] - 6s 30ms/step - loss: 0.6932 - accuracy: 0.8300 - f1_metric: 0.2504\n",
      "Epoch 94/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6933 - accuracy: 0.1612 - f1_metric: 0.2494\n",
      "Epoch 95/100\n",
      "187/187 [==============================] - 6s 33ms/step - loss: 0.6933 - accuracy: 0.6965 - f1_metric: 0.2490\n",
      "Epoch 96/100\n",
      "187/187 [==============================] - 8s 41ms/step - loss: 0.6932 - accuracy: 0.8555 - f1_metric: 0.2504\n",
      "Epoch 97/100\n",
      "187/187 [==============================] - 6s 31ms/step - loss: 0.6932 - accuracy: 0.5654 - f1_metric: 0.2496\n",
      "Epoch 98/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6932 - accuracy: 0.8127 - f1_metric: 0.2507\n",
      "Epoch 99/100\n",
      "187/187 [==============================] - 6s 30ms/step - loss: 0.6932 - accuracy: 0.7187 - f1_metric: 0.2488\n",
      "Epoch 100/100\n",
      "187/187 [==============================] - 5s 29ms/step - loss: 0.6933 - accuracy: 0.4708 - f1_metric: 0.2492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af3418a490>"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(32, 3, padding='same', input_shape=(300, 1)),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Conv1D(64, 3, padding='same'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Conv1D(128, 3, padding='same'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.1),\n",
    "    MaxPooling1D(2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    \n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy', f1_metric])\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y2_train), y=y2_train)\n",
    "\n",
    "\n",
    "class_weights_dict = {i : class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "model.fit(X_train, y2_train, epochs=100, batch_size=64, class_weight=class_weights_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_f1 = model.evaluate(X_test, y2_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "393/393 [==============================] - 6s 11ms/step - loss: 1.0645 - accuracy: 0.5059 - f1_metric: 0.5464\n",
      "Epoch 2/20\n",
      "393/393 [==============================] - 4s 11ms/step - loss: 1.0597 - accuracy: 0.5053 - f1_metric: 0.5464\n",
      "Epoch 3/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 1.0350 - accuracy: 0.5102 - f1_metric: 0.5460\n",
      "Epoch 4/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 1.0215 - accuracy: 0.5118 - f1_metric: 0.5467\n",
      "Epoch 5/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 1.0073 - accuracy: 0.5166 - f1_metric: 0.5466\n",
      "Epoch 6/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 1.0104 - accuracy: 0.5149 - f1_metric: 0.5465\n",
      "Epoch 7/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 0.9682 - accuracy: 0.5198 - f1_metric: 0.5461\n",
      "Epoch 8/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 0.9844 - accuracy: 0.5118 - f1_metric: 0.5469\n",
      "Epoch 9/20\n",
      "393/393 [==============================] - 4s 11ms/step - loss: 0.9823 - accuracy: 0.5170 - f1_metric: 0.5467\n",
      "Epoch 10/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 0.9570 - accuracy: 0.5187 - f1_metric: 0.5465\n",
      "Epoch 11/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 0.9384 - accuracy: 0.5229 - f1_metric: 0.5465\n",
      "Epoch 12/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 0.9302 - accuracy: 0.5205 - f1_metric: 0.5462\n",
      "Epoch 13/20\n",
      "393/393 [==============================] - 4s 11ms/step - loss: 0.9415 - accuracy: 0.5187 - f1_metric: 0.5462\n",
      "Epoch 14/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 0.9201 - accuracy: 0.5229 - f1_metric: 0.5464\n",
      "Epoch 15/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 0.9041 - accuracy: 0.5225 - f1_metric: 0.5467\n",
      "Epoch 16/20\n",
      "393/393 [==============================] - 4s 11ms/step - loss: 0.9097 - accuracy: 0.5221 - f1_metric: 0.5464\n",
      "Epoch 17/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 0.9003 - accuracy: 0.5277 - f1_metric: 0.5464\n",
      "Epoch 18/20\n",
      "393/393 [==============================] - 4s 11ms/step - loss: 0.9030 - accuracy: 0.5204 - f1_metric: 0.5466\n",
      "Epoch 19/20\n",
      "393/393 [==============================] - 4s 10ms/step - loss: 0.8900 - accuracy: 0.5237 - f1_metric: 0.5465\n",
      "Epoch 20/20\n",
      "393/393 [==============================] - 4s 11ms/step - loss: 0.8914 - accuracy: 0.5243 - f1_metric: 0.5465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af38a98f70>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout, BatchNormalization\n",
    "\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "x = Dense(128, activation='relu')(model.layers[-2].output)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "output_layer = Dense(2, activation='softmax')(x)\n",
    "\n",
    "new_model = Model(inputs=model.input, outputs=output_layer)\n",
    "\n",
    "\n",
    "new_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy', f1_metric])\n",
    "\n",
    "class_weights1 = compute_class_weight(class_weight='balanced', classes=np.unique(y1_train), y=y1_train)\n",
    "class_weights_dict1 = {i : class_weights1[i] for i in range(len(class_weights1))}\n",
    "\n",
    "\n",
    "new_model.fit(X_train1, y1_train, epochs=20, batch_size=64, class_weight=class_weights_dict1)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197/197 - 2s - loss: 0.6961 - accuracy: 0.5821 - f1_metric: 0.5436 - 2s/epoch - 11ms/step\n",
      "\n",
      "Test accuracy: 0.5821133255958557\n",
      "Test F1 Score: 0.543595552444458\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc, test_f1 = new_model.evaluate(X_test1, y1_test, verbose=2)\n",
    "print(\"\\nTest accuracy:\", test_acc)\n",
    "print(\"Test F1 Score:\", test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = kaggle_set['text'].tolist()\n",
    "\n",
    "max_length = max([len(seq) for seq in X4])\n",
    "\n",
    "word2vec = Word2Vec(X4, vector_size=300, window=10, min_count=1, workers=4)\n",
    "\n",
    "kaggle_set_new = np.array([get_text_vector(text, word2vec) for text in X4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_set_new.reshape((1000, 300, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = new_model.predict(kaggle_set_new)\n",
    "predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'id': range(len(predictions)), 'class': predictions})\n",
    "\n",
    "df.to_csv('predictions2.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
